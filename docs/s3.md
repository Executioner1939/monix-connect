---
id: s3
title: AWS S3
---

## Introduction

_AWS Simple Storage Service_ ([S3](https://aws.amazon.com/s3/?nc=sn&loc=0)) is an object storage service that offers industry leading scalability, availability, security and performance.
It allows data storage of any amount of data, commonly used as a data lake for big data applications which can now be easily integrated with monix.
   
## Dependency
 
 Add the following dependency:
 
 ```scala
 libraryDependencies += "io.monix" %% "monix-s3" % "0.1.0"
 ```

## Getting started 


 First of all, we need to create the s3 client that will allow us to authenticate and create an channel between our 
 application and the AWS S3 service. 
 
 ### asyncClient
 
  
 This module has been implemented using the `S3AsyncClient` from the [aws sdk](https://sdk.amazonaws.com/java/api/latest/software/amazon/awssdk/services/s3/model/package-summary.html), since it only exposes non blocking methods. 
 So, all of the methods exposed under `monix.connect.s3.S3` object would expect an implicit instance of 
 this async client class to be in the scope of the call.
 
 Below code shows an example on how to set up this connection. 
 Note that in this case the authentication is using AWS access and secret keys, but you might use another method such an _IAM_ role.
 
 ```scala
import java.net.URI
import software.amazon.awssdk.auth.credentials.{AwsBasicCredentials, StaticCredentialsProvider}
import software.amazon.awssdk.services.s3.S3AsyncClient
import software.amazon.awssdk.regions.Region.AWS_GLOBAL

val basicAWSCredentials: AwsBasicCredentials = AwsBasicCredentials.create(s3AccessKey, s3SecretKey)
val credentialsProvider: StaticCredentialsProvider = StaticCredentialsProvider.create(basicAWSCredentials)

//note that the client is defined as implicit, this is on purpose since each of the methods defined in
//the monix s3 connector will expect that.
 implicit val s3Client: S3AsyncClient = S3AsyncClient
    .builder()
    .credentialsProvider(credentialsProvider)
    .region(AWS_GLOBAL)
    .endpointOverride(URI.create(endPoint)) //this one is used to point to the localhost s3 service, not used in prod 
    .build
```


  ### createBucket 

Once you have configured the client, you would probably need to create a bucket:

 ```scala
import software.amazon.awssdk.services.s3.model.CreateBucketResponse

val bucketName: String = "myBucket" 
val t: Task[CreateBucketResponse] = S3.createBucket(bucketName)
```
  ### deleteBucket 

 On the other hand if you want to remove the bucket:
 
 ```scala
import software.amazon.awssdk.services.s3.model.DeleteBucketResponse

val bucketName: String = "myBucket" 
val t: Task[DeleteBucketResponse] = S3.deleteBucket(bucketName)
```

  ### putObject

You can also easily create and write into an S3 object with put object operation.
Note that if you need to update large amount of data you should not be using this method, see instead [multipartUpload](###m)`.

 ```scala
import software.amazon.awssdk.services.s3.model.PutObjectResponse

val content: Array[Byte] = "file content".getBytes()
val t: Task[PutObjectResponse] = S3.putObject(bucketName, objectKey, content)
}
```

  ### deleteObject
  
You can also operate at object level within a bucket with:
 ```scala
import software.amazon.awssdk.services.s3.model.{DeleteObjectResponse, ListObjectsResponse}

val t: Task[DeleteObjectResponse] = S3.deleteObject(bucketName)
```

  ### listObject
  
Lists all the objects within a bucket:

 ```scala
import software.amazon.awssdk.services.s3.model.{DeleteObjectResponse, ListObjectsResponse}

val _: Task[ListObjectsResponse] = S3.listObjects(bucketName)
```

  ### getObject
  
Download the given S3 object as a single byte array. 
Note that this operation is dangerous to perform on large objects, `multipartDownload` would be supported in future releases to 
support those use cases.

 ```scala
val objectKey: String = "/object/file.txt"
val _: Task[Array[Byte]] = S3.getObject(bucketName, objectKey)

```

### multipartUpload

Finally, for dealing with large files of data you might want to use the `multipartUpload` implementation.
This one can be used to consume an observable of bytes that would send a partial upload request for each received element if it was bigger than the minimum size, otherwise the chunk will be concatenated on the next request. 

Thus, it reduces substantially the risk on having _OOM_ errors or getting http requests failures, 
since the whole file does not need to be allocated in the memory and the http request body won't be that big because it would have been done by parts. 

Note that the method can be tuned with specific aws configurations such as `acl's`, `requestPayer`, etc. But a very important one to have present is by the minimum `chunksize` that will be sent, being 5MB the default and minimum size (lower values would result in failure).

```scala
import software.amazon.awssdk.services.s3.model.CompleteMultipartUploadResponse

// given an stream of array bytes 
val ob: Observable[Array[Byte]] = Observable.fromIterable(chunks)

// and a multipart upload consumer
val multipartUploadConsumer: Consumer[Array[Byte], Task[CompleteMultipartUploadResponse]] =
  S3.multipartUpload(bucketName, objectKey)

// then
ob.fromIterable(chunks).consumeWith(multipartUploadConsumer)
```

## Local testing

In order test `AWS S3` localy, we used a docker image provided by [minio](https://github.com/minio/minio).

Add the following service description to your `docker-compose.yaml` file:

```yaml
minio:
  image: minio/minio
  ports:
    - "9000:9000"
  volumes:
    - ./minio/data:/data
  environment:
    - MINIO_ACCESS_KEY=TESTKEY
    - MINIO_SECRET_KEY=TESTSECRET
  healthcheck:
    test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
    interval: 35s
    timeout: 20s
    retries: 3
  command: server --compat /data
```

Then, run the following command to build and start the S3 service:

```shell script
docker-compose -f docker-compose.yml up -d minio
``` 
Check out that the service has started correctly, notice that a healthcheck has been defined on the description of the minio service, 
that's because it is a heavy image and sometimes it takes bit long to start or sometimes it even fails, so that would prevent that.

Finally, create the connection with AWS S3, note that minio does not has support for `AnonymousCredentialsProvider`, 
therefore you'll have to use `AwsBasicCredentials`, in which the _key_ and _secret_ will correspond respectively to the
 defined environment variables from docker compose definition `MINIO_ACCESS_KEY` and `MINIO_SECRET_KEY`. 

```scala
import software.amazon.awssdk.regions.Region.AWS_GLOBAL
import software.amazon.awssdk.services.s3.S3AsyncClient
import software.amazon.awssdk.auth.credentials.{AwsBasicCredentials, StaticCredentialsProvider}

val minioEndPoint: String = "http://localhost:9000"

val s3AccessKey: String = "TESTKEY" //has to be equal to the `MINIO_ACCESS_KEY` env var value 
val s3SecretKey: String = "TESTSECRET" //has to be equal to the `MINIO_SECRET_KEY` env var value 

val basicAWSCredentials = AwsBasicCredentials.create(s3AccessKey, s3SecretKey)
implicit val s3AsyncClient: S3AsyncClient = S3AsyncClient
  .builder()
  .credentialsProvider(StaticCredentialsProvider.create(basicAWSCredentials))
  .region(AWS_GLOBAL)
  .endpointOverride(URI.create(minioEndPoint))
  .build
```

Now you are ready to run your application! 

_Note that the above example defines the client as `implicit`, since it is how the api will expect this one._
